{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import types\n",
    "\n",
    "from config import redshift_coltype_dir, query_output_dir\n",
    "\n",
    "user = os.environ['USER']\n",
    "sys.path.insert(0, '/Users/{}/Box/DataViz Projects/Utility Code'.format(user))\n",
    "from utils_io import *\n",
    "\n",
    "sys.path.insert(0, '/Users/{}/Box/DataViz Projects/Application Secure Files'.format(user))\n",
    "from creds import SOCRATA_CREDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcels 2018\n",
    "\n",
    "Create basis.parcels_2018 from the [Parcels 2018](https://data.bayareametro.gov/Cadastral/Parcels-2018/fqea-xb6g) data on Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'mtc-basis'\n",
    "s3_key = 'Parcels2018_Socrata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pull Parcels 2018 data from Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the data from Socrata\n",
    "endpoint = SOCRATA_CREDS['domain']\n",
    "username = SOCRATA_CREDS[user]['username']\n",
    "password = SOCRATA_CREDS[user]['password']\n",
    "app_token = SOCRATA_CREDS[user]['app_token']\n",
    "\n",
    "df = pull_df_from_socrata(endpoint, username, password, app_token)\n",
    "# this column is too long for max Redshift column length (65,535 when stored as varchar)\n",
    "df.drop('the_geom', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Push Parcels 2018 data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post to S3 \n",
    "post_df_to_s3(df, bucket=s3_bucket, key=s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Parcels 2018 data into Redshift from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from S3 if you want to start from there\n",
    "df = download_df_from_s3(s3_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post to Redshift\n",
    "s3_path = 's3://{}/{}'.format(s3_bucket, s3_key)\n",
    "tablename = 'basis.parcels_2018'\n",
    "\n",
    "# create conservative lengths\n",
    "ctype_override = {'joinid': types.VARCHAR(50),\n",
    "                   'apn': types.VARCHAR(100),\n",
    "                   'jurisdict': types.VARCHAR(100),\n",
    "                   'fipco': types.VARCHAR(50)}\n",
    "\n",
    "column_type_dict = create_redshift_column_type_dict(df)\n",
    "column_type_dict.update(ctype_override)\n",
    "\n",
    "coltype_fname = os.path.join(redshift_coltype_dir, 'parcels_2018_coltypes.txt')\n",
    "write_text(repr(column_type_dict), coltype_fname)\n",
    "create_table_command = create_redshift_table_via_s3(tablename, s3_path, dbname='dev', column_type_dict=column_type_dict)\n",
    "create_table_fname = os.path.join(query_output_dir, 'init_basis.parcels_2018.sql')\n",
    "write_text(create_table_command, create_table_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
